# Dysruption Consensus Verifier Agent (CVA) Configuration
# Version: 2.0 - Rate Limit Resilient Edition

# LLM Configuration - 99% Uptime Model Ensemble
llms:
  # Architect/Logic Judge - Gemini 2.0 Flash (COST-EFFECTIVE MODE)
  architect:
    model: "gemini/gemini-2.0-flash-exp"
    provider: "google"
    max_tokens: 4096
    temperature: 0.0  # Deterministic for consistency
    cost_per_1k_input: 0.000075
    cost_per_1k_output: 0.0003
    role: "Senior Architect"
    focus: "logic, structure, efficiency, modularity"
  
  # Security/Efficiency Judge - DeepSeek V3 Native API (NO RATE LIMITS)
  # Direct DeepSeek API = no Groq rate limit issues
  security:
    model: "deepseek/deepseek-chat"
    provider: "deepseek"
    max_tokens: 4096
    temperature: 0.0
    cost_per_1k_input: 0.00014
    cost_per_1k_output: 0.00028
    role: "Security Expert"
    focus: "vulnerabilities, OWASP risks, hardcoded secrets"
  
  # User Proxy/Vibe Judge - Gemini 2.0 Flash (Best available for spec alignment)
  user_proxy:
    model: "gemini/gemini-2.0-flash-exp"
    provider: "google"
    max_tokens: 4096
    temperature: 0.1
    cost_per_1k_input: 0.00125
    cost_per_1k_output: 0.005
    role: "User Advocate"
    focus: "usability, requirement matching, user intent"
  
  # Extraction LLM - Gemini 2.0 Flash (Fast, cheap for parsing)
  extraction:
    model: "gemini/gemini-2.0-flash-exp"
    provider: "google"
    max_tokens: 4096
    temperature: 0.1
    cost_per_1k_input: 0.000075
    cost_per_1k_output: 0.0003
    role: "Requirements Analyst"
    focus: "exhaustive extraction, schema validation"
  
  # Remediation LLM - GPT-4o-mini (Best for code fixes)
  remediation:
    model: "openai/gpt-4o-mini"
    provider: "openai"
    max_tokens: 8192
    temperature: 0.2
    cost_per_1k_input: 0.00015
    cost_per_1k_output: 0.0006
    role: "Code Fixer"
    focus: "generate patches, fix flaws"
  
  # Prompt Synthesizer LLM - Gemini 2.0 Flash (COST-EFFECTIVE MODE)
  synthesizer:
    model: "gemini/gemini-2.0-flash-exp"
    provider: "google"
    max_tokens: 8192
    temperature: 0.3  # Slightly creative for prompt generation
    cost_per_1k_input: 0.000075
    cost_per_1k_output: 0.0003
    role: "Prompt Engineer"
    focus: "synthesize actionable fix prompts from verdict"
  
  # Fallback models (used when primary fails)
  fallback:
    primary: "openai/gpt-4o"
    secondary: "deepseek/deepseek-chat"
    tertiary: "gemini/gemini-2.0-flash-exp"

# Thresholds
thresholds:
  pass_score: 7  # Minimum score out of 10 to pass
  consensus_ratio: 0.67  # 2/3 majority required
  confidence_min: 0.7  # Minimum confidence for valid judgment
  min_invariants: 5  # Minimum invariants to extract from spec
  chunk_size_tokens: 10000  # Max tokens per chunk
  context_window: 128000  # Max context window
  human_alignment_target: 0.85  # 85% target alignment with human judgments
  veto_confidence: 0.85  # Security judge veto threshold (raised from 0.8 to reduce false positives)

# Timeouts Configuration (in seconds)
# All timeout values are centralized here for easy tuning
timeouts:
  llm_request_seconds: 60      # Single LLM API call timeout
  llm_batch_seconds: 300       # Batch LLM operations (multi-model tribunal)
  static_pylint_seconds: 30    # Pylint analysis timeout per file
  static_bandit_seconds: 30    # Bandit security scan timeout
  self_heal_verify_seconds: 300  # Self-heal verification loop timeout
  preflight_ping_seconds: 12   # API key validation ping timeout
  watcher_debounce_seconds: 15 # File watcher debounce (also in watcher section)
  extraction_seconds: 120      # Spec/constitution extraction timeout

# Environment Variable Keys
env_keys:
  anthropic: "ANTHROPIC_API_KEY"
  google: "GOOGLE_API_KEY"
  groq: "GROQ_API_KEY"
  openai: "OPENAI_API_KEY"
  deepseek: "DEEPSEEK_API_KEY"

# Watcher Configuration
watcher:
  debounce_seconds: 15
  supported_extensions:
    python: [".py"]
    javascript: [".js", ".ts", ".jsx", ".tsx"]
  ignore_patterns:
    - "__pycache__"
    - "node_modules"
    - ".git"
    - ".venv"
    - "venv"
    - "*.pyc"
    - "*.pyo"
    - ".env"
    - "*.log"

# Static Analysis
static_analysis:
  enabled: false  # Disabled for self-verification to speed up pipeline
  fail_fast: false  # Disabled for self-verification
  # Paths excluded from static analysis (relative to project root)
  exclude_paths:
    - "sample_project/"
    - "tests/"
    - "__pycache__/"
    - ".venv/"
  pylint:
    enabled: false
    max_line_length: 120
    # Disable false positive codes:
    # C0114/C0115/C0116 - Missing docstrings (style preference)
    # E0401 - import-error (environment-specific, not a code defect)
    # E0611 - no-name-in-module (environment-specific)
    # W0611 - unused-import (often intentional in __init__.py)
    # R0801 - duplicate-code (cross-file, not per-file relevant)
    disable: ["C0114", "C0115", "C0116", "E0401", "E0611", "W0611", "R0801"]
  bandit:
    enabled: true
    severity: "low"  # low, medium, high
    # Skip patterns for bandit (test/sample files often have intentional issues)
    skip_patterns:
      - "sample_project/*"
      - "tests/*"

# Remediation
remediation:
  enabled: false  # Set to true to enable fix suggestions
  max_fixes_per_file: 5

# Retry Configuration - Enhanced for Rate Limit Resilience
retry:
  max_attempts: 5  # Increased from 3
  backoff_seconds: 2
  backoff_multiplier: 2  # Exponential backoff: 2s, 4s, 8s, 16s, 32s
  max_backoff_seconds: 60  # Cap at 1 minute
  jitter_range: [1, 10]  # Random jitter between 1-10s
  fallback_enabled: true
  retry_on_errors:
    - "RateLimitError"
    - "GroqException"
    - "AnthropicException"
    - "APIError"
    - "ServiceUnavailable"
  credit_low_threshold: 5.0  # Alert when credit drops below $5

# Output
output:
  report_file: "REPORT.md"
  verdict_file: "verdict.json"
  sarif_file: "verdict.sarif"
  sarif_enabled: true  # Enable SARIF export for GitHub Code Scanning
  criteria_file: "criteria.json"
  verbose: true
  color_coded: true  # Green/Red sections in report

# Async Configuration
async_config:
  enabled: true
  max_concurrent_calls: 5
  timeout_seconds: 60

# Fallback Configuration - Multi-tier fallback chain
fallback:
  enabled: true
  models:
    - "openai/gpt-4o"  # Primary fallback: GPT-4o
    - "deepseek/deepseek-chat"  # Secondary: DeepSeek (no rate limits)
    - "gemini/gemini-2.0-flash-exp"  # Tertiary: Gemini Flash
  # Legacy field for backwards compatibility
  model: "openai/gpt-4o"

# Redis Caching (LiteLLM)
cache:
  enabled: true
  type: "redis"  # Options: redis, local, none
  host: "localhost"
  port: 6379
  ttl_seconds: 3600  # Cache responses for 1 hour
  namespace: "cva_llm_cache"

# Credit Monitoring
credits:
  monitor_enabled: true
  alert_thresholds:
    anthropic: 5.0  # Alert when Anthropic credit < $5
    openai: 10.0
  check_interval_seconds: 300  # Check every 5 minutes
