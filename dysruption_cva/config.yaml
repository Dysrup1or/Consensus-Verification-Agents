# Dysruption Consensus Verifier Agent (CVA) Configuration
# Version: 2.0 - Rate Limit Resilient Edition

# LLM Configuration - 99% Uptime Model Ensemble
llms:
  # Architect/Logic Judge - Claude Sonnet 4 (Premium architecture analysis)
  architect:
    model: "anthropic/claude-sonnet-4-20250514"
    provider: "anthropic"
    max_tokens: 4096
    temperature: 0.0  # Deterministic for consistency
    cost_per_1k_input: 0.015
    cost_per_1k_output: 0.075
    role: "Senior Architect"
    focus: "logic, structure, efficiency, modularity"
  
  # Security/Efficiency Judge - DeepSeek V3 Native API (NO RATE LIMITS)
  # Direct DeepSeek API = no Groq rate limit issues
  security:
    model: "deepseek/deepseek-chat"
    provider: "deepseek"
    max_tokens: 4096
    temperature: 0.0
    cost_per_1k_input: 0.00014
    cost_per_1k_output: 0.00028
    role: "Security Expert"
    focus: "vulnerabilities, OWASP risks, hardcoded secrets"
  
  # User Proxy/Vibe Judge - Gemini 2.0 Flash (Best available for spec alignment)
  user_proxy:
    model: "gemini/gemini-2.0-flash-exp"
    provider: "google"
    max_tokens: 4096
    temperature: 0.1
    cost_per_1k_input: 0.00125
    cost_per_1k_output: 0.005
    role: "User Advocate"
    focus: "usability, requirement matching, user intent"
  
  # Extraction LLM - Gemini 2.0 Flash (Fast, cheap for parsing)
  extraction:
    model: "gemini/gemini-2.0-flash-exp"
    provider: "google"
    max_tokens: 4096
    temperature: 0.1
    cost_per_1k_input: 0.000075
    cost_per_1k_output: 0.0003
    role: "Requirements Analyst"
    focus: "exhaustive extraction, schema validation"
  
  # Remediation LLM - GPT-4o-mini (Best for code fixes)
  remediation:
    model: "openai/gpt-4o-mini"
    provider: "openai"
    max_tokens: 8192
    temperature: 0.2
    cost_per_1k_input: 0.00015
    cost_per_1k_output: 0.0006
    role: "Code Fixer"
    focus: "generate patches, fix flaws"
  
  # Prompt Synthesizer LLM - Claude Sonnet 4 (Best for prompt engineering)
  synthesizer:
    model: "anthropic/claude-sonnet-4-20250514"
    provider: "anthropic"
    max_tokens: 8192
    temperature: 0.3  # Slightly creative for prompt generation
    cost_per_1k_input: 0.015
    cost_per_1k_output: 0.075
    role: "Prompt Engineer"
    focus: "synthesize actionable fix prompts from verdict"
  
  # Fallback models (used when primary fails)
  fallback:
    primary: "openai/gpt-4o"
    secondary: "deepseek/deepseek-chat"
    tertiary: "gemini/gemini-2.0-flash-exp"

# Thresholds
thresholds:
  pass_score: 7  # Minimum score out of 10 to pass
  consensus_ratio: 0.67  # 2/3 majority required
  confidence_min: 0.7  # Minimum confidence for valid judgment
  min_invariants: 5  # Minimum invariants to extract from spec
  chunk_size_tokens: 10000  # Max tokens per chunk
  context_window: 128000  # Max context window
  human_alignment_target: 0.85  # 85% target alignment with human judgments

# Environment Variable Keys
env_keys:
  anthropic: "ANTHROPIC_API_KEY"
  google: "GOOGLE_API_KEY"
  groq: "GROQ_API_KEY"
  openai: "OPENAI_API_KEY"
  deepseek: "DEEPSEEK_API_KEY"

# Watcher Configuration
watcher:
  debounce_seconds: 15
  supported_extensions:
    python: [".py"]
    javascript: [".js", ".ts", ".jsx", ".tsx"]
  ignore_patterns:
    - "__pycache__"
    - "node_modules"
    - ".git"
    - ".venv"
    - "venv"
    - "*.pyc"
    - "*.pyo"
    - ".env"
    - "*.log"

# Static Analysis
static_analysis:
  enabled: true
  fail_fast: false  # Disabled for testing - allows full tribunal evaluation
  pylint:
    enabled: true
    max_line_length: 120
    disable: ["C0114", "C0115", "C0116"]  # Disable docstring warnings
  bandit:
    enabled: true
    severity: "low"  # low, medium, high

# Remediation
remediation:
  enabled: false  # Set to true to enable fix suggestions
  max_fixes_per_file: 5

# Retry Configuration - Enhanced for Rate Limit Resilience
retry:
  max_attempts: 5  # Increased from 3
  backoff_seconds: 2
  backoff_multiplier: 2  # Exponential backoff: 2s, 4s, 8s, 16s, 32s
  max_backoff_seconds: 60  # Cap at 1 minute
  jitter_range: [1, 10]  # Random jitter between 1-10s
  fallback_enabled: true
  retry_on_errors:
    - "RateLimitError"
    - "GroqException"
    - "AnthropicException"
    - "APIError"
    - "ServiceUnavailable"
  credit_low_threshold: 5.0  # Alert when credit drops below $5

# Output
output:
  report_file: "REPORT.md"
  verdict_file: "verdict.json"
  criteria_file: "criteria.json"
  verbose: true
  color_coded: true  # Green/Red sections in report

# Async Configuration
async_config:
  enabled: true
  max_concurrent_calls: 5
  timeout_seconds: 60

# Fallback Configuration - Multi-tier fallback chain
fallback:
  enabled: true
  models:
    - "openai/gpt-4o"  # Primary fallback: GPT-4o
    - "deepseek/deepseek-chat"  # Secondary: DeepSeek (no rate limits)
    - "gemini/gemini-2.0-flash-exp"  # Tertiary: Gemini Flash
  # Legacy field for backwards compatibility
  model: "openai/gpt-4o"

# Redis Caching (LiteLLM)
cache:
  enabled: true
  type: "redis"  # Options: redis, local, none
  host: "localhost"
  port: 6379
  ttl_seconds: 3600  # Cache responses for 1 hour
  namespace: "cva_llm_cache"

# Credit Monitoring
credits:
  monitor_enabled: true
  alert_thresholds:
    anthropic: 5.0  # Alert when Anthropic credit < $5
    openai: 10.0
  check_interval_seconds: 300  # Check every 5 minutes
